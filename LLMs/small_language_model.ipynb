{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ef966e",
   "metadata": {},
   "source": [
    "# Small Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccead320",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b4bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import GPT2Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d2e95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cac395",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05801bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length, tokenizer):\n",
    "        self.seq_length = seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = tokenizer.encode(text)\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index + self.seq_length]\n",
    "        y = self.data[index + 1:index + self.seq_length + 1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d0b8f",
   "metadata": {},
   "source": [
    "## Example steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a7c25",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe7305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212, 21554,  9405,   262,  4688,   761]]) torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "x_text = \"This thesis addresses the critical need\"\n",
    "x = tokenizer(x_text, return_tensors=\"pt\").input_ids\n",
    "print(x, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b83e47",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc7484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "embed_size = 64\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim=embed_size)\n",
    "x_emb = embedding(x)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c79562",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2d3828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "print(positional_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12f03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "x_emb_pos_encoding = x_emb + positional_encoding[:, :x_emb.shape[1], :]\n",
    "print(x_emb_pos_encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f54390",
   "metadata": {},
   "source": [
    "## Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "214481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "hidden_dim = 128\n",
    "\n",
    "transformer_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=embed_size,\n",
    "    nhead=num_heads,\n",
    "    dim_feedforward=hidden_dim,\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476f7485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33472\n"
     ]
    }
   ],
   "source": [
    "num_weights = 0\n",
    "for p in transformer_layer.parameters():\n",
    "    num_weights += p.numel()\n",
    "print(num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2b1d835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(x_emb.shape[1], x_emb.shape[1]) * float(\"-inf\"), diagonal=1).to(x.device)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90b97dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 64])\n"
     ]
    }
   ],
   "source": [
    "x_contextual_emb = transformer_layer(x_emb_pos_encoding, mask)\n",
    "print(x_contextual_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f63a86",
   "metadata": {},
   "source": [
    "### Final Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f82d12fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(embed_size, vocab_size)\n",
    "x_logits = fc(x_contextual_emb)\n",
    "print(x_logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36798195",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c8de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1212, 21554,  9405,   262,  4688,   761,   329,  4050, 40050, 46254]]) torch.Size([1, 10])\n",
      "tensor([[21554,  9405,   262,  4688,   761,   329,  4050, 40050, 46254,   290]]) torch.Size([1, 10])\n",
      "['This thesis addresses the critical need for effective Fault Detection']\n",
      "[' thesis addresses the critical need for effective Fault Detection and']\n"
     ]
    }
   ],
   "source": [
    "text = \"This thesis addresses the critical need for effective Fault Detection and Isolation (FDI) in green hydrogen (GH2) production, \" \\\n",
    "\"a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. \" \\\n",
    "\"Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. \" \\\n",
    "\"Additionally, explainability in Hybrid-FDI is often overlooked. The proposed hybrid approach aims to be efficient in data usage and \" \\\n",
    "\"explainable, leveraging physics-based models and Artificial Intelligence (AI). This study introduces Bond Graph-Convolutional Neural Net \" \\\n",
    "\"(BG-CNN), a novel hybrid FDI method addressing AI model training challenges for fault diagnosis. BG-CNN combines BG residual generation \" \\\n",
    "\"and CNN-based fault classification, particularly in scenarios with limited labeled data. Additionally, a Self-Supervised Learning (SSL) \" \\\n",
    "\"method enhances FDI in such situations. The study also discusses Bond Graph-eXplainable AI (BG-XAI), an occlusion-based method, \" \\\n",
    "\"emphasizing the importance of meaningful explanations for fault predictions, showcasing its effectiveness through visualizations. \" \\\n",
    "\"The BG-CNN method with SSL was employed for the FDI of the Proton Exchange Membrane (PEM) electrolyzer and railway tracks, \" \\\n",
    "\"surpassing the performance of traditional methods. Comparative analysis demonstrated the superior performance of the proposed method, \" \\\n",
    "\"particularly in scenarios with limited labeled data, outperforming state-of-the-art SSL methods. The BG-XAI method was used to \" \\\n",
    "\"provide explanations for predictions in accordance with structural analysis.\"\n",
    "\n",
    "tokenized_text = tokenizer(text, return_tensors='pt').input_ids\n",
    "\n",
    "seq_length = 10\n",
    "x = tokenized_text[:,:seq_length]\n",
    "y = tokenized_text[:,1:seq_length+1]\n",
    "\n",
    "print(x, x.shape)\n",
    "print(y, y.shape)\n",
    "\n",
    "print(tokenizer.batch_decode(x))\n",
    "print(tokenizer.batch_decode(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ccbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "batch_size = 16\n",
    "\n",
    "dataset = TextDataset(text, seq_length, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59238ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "tensor([[ 9385, 46044, 21010, 26934,  1942, 22825,  8601,  9184, 32086, 31447]])\n",
      "tensor(11.0865, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_emb = embedding(x)\n",
    "x_emb_pos_encoding = x_emb + positional_encoding[:, :x_emb.shape[1], :]\n",
    "mask = torch.triu(torch.ones(x_emb.shape[1], x_emb.shape[1]) * float(\"-inf\"), diagonal=1).to(x.device)\n",
    "x_contextual_emb = transformer_layer(x_emb_pos_encoding, mask)\n",
    "x_logits = fc(x_contextual_emb)\n",
    "\n",
    "print(x_logits.shape)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(x_logits.argmax(dim=-1))\n",
    "\n",
    "loss = loss_fn(x_logits.view(-1, dataset.vocab_size), y.view(-1))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501373b",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd255680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, seq_length):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, embed_size))\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        seq_length = x.size(1)\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n",
    "        x = self.encoder_layer(x, src_mask = mask)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab50a12",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be769109",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_size=embed_size,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    seq_length=seq_length\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb4ff5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d614d0e2d4444743b9b9809c7d840afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 10.3945\n",
      "Epoch 2/20, Loss: 8.6460\n",
      "Epoch 3/20, Loss: 6.2310\n",
      "Epoch 4/20, Loss: 4.4847\n",
      "Epoch 5/20, Loss: 3.6225\n",
      "Epoch 6/20, Loss: 2.9358\n",
      "Epoch 7/20, Loss: 2.3371\n",
      "Epoch 8/20, Loss: 1.8033\n",
      "Epoch 9/20, Loss: 1.3875\n",
      "Epoch 10/20, Loss: 1.0885\n",
      "Epoch 11/20, Loss: 0.8781\n",
      "Epoch 12/20, Loss: 0.7237\n",
      "Epoch 13/20, Loss: 0.6022\n",
      "Epoch 14/20, Loss: 0.5258\n",
      "Epoch 15/20, Loss: 0.4494\n",
      "Epoch 16/20, Loss: 0.4023\n",
      "Epoch 17/20, Loss: 0.3540\n",
      "Epoch 18/20, Loss: 0.3278\n",
      "Epoch 19/20, Loss: 0.2996\n",
      "Epoch 20/20, Loss: 0.2738\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length) * float(\"-inf\"), diagonal=1).to(x.device)\n",
    "        output = model(x, mask)\n",
    "        loss = criterion(output.view(-1, dataset.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e541ad3",
   "metadata": {},
   "source": [
    "## Infernece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf4150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearted Text: a key player in mitigating the greenhouse effect. To tackle this challenge, this thesis introduces a hybrid strategy for FDI. Extensive reviews of FDI algorithms reveal a gap in existing literature, emphasizing accuracy but neglecting the need for labeled data. Additionally, explainability in Hybrid-FDI\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "model.eval()\n",
    "input_text = \"a key player in mitigating the greenhouse effect\"\n",
    "input_seq = torch.tensor(tokenizer.encode(input_text), dtype=torch.long).unsqueeze(0)\n",
    "generated = input_text\n",
    "\n",
    "for _ in range(50):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_seq)\n",
    "        next_token = output.argmax(dim=-1)[0, -1].item()\n",
    "        generated += tokenizer.decode([next_token])\n",
    "        input_seq = torch.cat([input_seq, torch.tensor([[next_token]])], dim=1)\n",
    "        input_seq = input_seq[:, -seq_length:]\n",
    "        # break\n",
    "\n",
    "print(\"Genearted Text:\", generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
